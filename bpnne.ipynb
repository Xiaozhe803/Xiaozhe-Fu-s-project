{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from copy import deepcopy\n",
    "import random\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data\n",
    "data=pd.read_table(\"./features.txt\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TextID</th>\n",
       "      <th>URL</th>\n",
       "      <th>Label</th>\n",
       "      <th>totalWordsCount</th>\n",
       "      <th>semanticobjscore</th>\n",
       "      <th>semanticsubjscore</th>\n",
       "      <th>CC</th>\n",
       "      <th>CD</th>\n",
       "      <th>DT</th>\n",
       "      <th>EX</th>\n",
       "      <th>...</th>\n",
       "      <th>pronouns2nd</th>\n",
       "      <th>pronouns3rd</th>\n",
       "      <th>compsupadjadv</th>\n",
       "      <th>past</th>\n",
       "      <th>imperative</th>\n",
       "      <th>present3rd</th>\n",
       "      <th>present1st2nd</th>\n",
       "      <th>sentence1st</th>\n",
       "      <th>sentencelast</th>\n",
       "      <th>txtcomplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Text0001</td>\n",
       "      <td>http://msn.foxsports.com/foxsoccer/mexico/stor...</td>\n",
       "      <td>objective</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Text0002</td>\n",
       "      <td>http://msn.foxsports.com/foxsoccer/premierleag...</td>\n",
       "      <td>objective</td>\n",
       "      <td>309</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Text0003</td>\n",
       "      <td>http://uk.eurosport.yahoo.com/04022011/58/fed-...</td>\n",
       "      <td>objective</td>\n",
       "      <td>149</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Text0004</td>\n",
       "      <td>http://uk.eurosport.yahoo.com/07022011/58/bund...</td>\n",
       "      <td>objective</td>\n",
       "      <td>305</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Text0005</td>\n",
       "      <td>http://uk.eurosport.yahoo.com/05022011/58/fed-...</td>\n",
       "      <td>objective</td>\n",
       "      <td>491</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Text0996</td>\n",
       "      <td>http://www.nba.com/pistons/news/blaha_090313.html</td>\n",
       "      <td>subjective</td>\n",
       "      <td>926</td>\n",
       "      <td>46</td>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>46</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>49</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Text0997</td>\n",
       "      <td>http://www.nba.com/pistons/news/smashing_succe...</td>\n",
       "      <td>subjective</td>\n",
       "      <td>876</td>\n",
       "      <td>48</td>\n",
       "      <td>26</td>\n",
       "      <td>9</td>\n",
       "      <td>109</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>43</td>\n",
       "      <td>5</td>\n",
       "      <td>34</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Text0998</td>\n",
       "      <td>http://www.foxsportswest.com/01/07/13/Hypocrit...</td>\n",
       "      <td>subjective</td>\n",
       "      <td>1469</td>\n",
       "      <td>82</td>\n",
       "      <td>53</td>\n",
       "      <td>14</td>\n",
       "      <td>171</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>49</td>\n",
       "      <td>12</td>\n",
       "      <td>40</td>\n",
       "      <td>19</td>\n",
       "      <td>51</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Text0999</td>\n",
       "      <td>http://www.football365.com/f365-features/84318...</td>\n",
       "      <td>subjective</td>\n",
       "      <td>343</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Text1000</td>\n",
       "      <td>http://www.nba.com/pistons/features/blaha_1103...</td>\n",
       "      <td>subjective</td>\n",
       "      <td>823</td>\n",
       "      <td>43</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       TextID                                                URL       Label  \\\n",
       "0    Text0001  http://msn.foxsports.com/foxsoccer/mexico/stor...   objective   \n",
       "1    Text0002  http://msn.foxsports.com/foxsoccer/premierleag...   objective   \n",
       "2    Text0003  http://uk.eurosport.yahoo.com/04022011/58/fed-...   objective   \n",
       "3    Text0004  http://uk.eurosport.yahoo.com/07022011/58/bund...   objective   \n",
       "4    Text0005  http://uk.eurosport.yahoo.com/05022011/58/fed-...   objective   \n",
       "..        ...                                                ...         ...   \n",
       "995  Text0996  http://www.nba.com/pistons/news/blaha_090313.html  subjective   \n",
       "996  Text0997  http://www.nba.com/pistons/news/smashing_succe...  subjective   \n",
       "997  Text0998  http://www.foxsportswest.com/01/07/13/Hypocrit...  subjective   \n",
       "998  Text0999  http://www.football365.com/f365-features/84318...  subjective   \n",
       "999  Text1000  http://www.nba.com/pistons/features/blaha_1103...  subjective   \n",
       "\n",
       "     totalWordsCount  semanticobjscore  semanticsubjscore  CC   CD  DT  EX  \\\n",
       "0                109                 0                  1   7    9   0   5   \n",
       "1                309                21                  4   1   19   1   4   \n",
       "2                149                 6                  1   8   14   0   5   \n",
       "3                305                18                  5   7   26   0  10   \n",
       "4                491                23                  8  33   47   0  12   \n",
       "..               ...               ...                ...  ..  ...  ..  ..   \n",
       "995              926                46                 34   5   83   1  20   \n",
       "996              876                48                 26   9  109   1  16   \n",
       "997             1469                82                 53  14  171   1  10   \n",
       "998              343                 7                  5   4   24   0   9   \n",
       "999              823                43                 22  19  100   1  13   \n",
       "\n",
       "     ...  pronouns2nd  pronouns3rd  compsupadjadv  past  imperative  \\\n",
       "0    ...            0            3              0    11           0   \n",
       "1    ...            0           10              0    13           0   \n",
       "2    ...            0            2              0     8           0   \n",
       "3    ...            0            8              3    13           1   \n",
       "4    ...            0           16              2    34           1   \n",
       "..   ...          ...          ...            ...   ...         ...   \n",
       "995  ...           16           46             10    13          12   \n",
       "996  ...            8           43              5    34           9   \n",
       "997  ...            9           49             12    40          19   \n",
       "998  ...            0            8              1     3           3   \n",
       "999  ...            9           18              6    45           7   \n",
       "\n",
       "     present3rd  present1st2nd  sentence1st  sentencelast  txtcomplexity  \n",
       "0             0              0            0             1             18  \n",
       "1            14              9            1             1             14  \n",
       "2             3              2            1             1             18  \n",
       "3             7              1            1             1             20  \n",
       "4             5              6            1             1             24  \n",
       "..          ...            ...          ...           ...            ...  \n",
       "995          49             29            1             1             24  \n",
       "996          19             31            1             1             21  \n",
       "997          51             42            1             1             18  \n",
       "998          25              7            1             1             11  \n",
       "999          16             16            1             1             23  \n",
       "\n",
       "[1000 rows x 62 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=data.columns.drop([\"Label\",\"WRB\",\"NNP\",\"TextID\",\"URL\",\"totalWordsCount\",\"semanticobjscore\",\"semanticsubjscore\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CC', 'CD', 'DT', 'EX', 'FW', 'INs', 'JJ', 'JJR', 'JJS', 'LS', 'MD',\n",
       "       'NN', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS',\n",
       "       'RP', 'SYM', 'TOs', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ',\n",
       "       'WDT', 'WP', 'WP$', 'baseform', 'Quotes', 'questionmarks',\n",
       "       'exclamationmarks', 'fullstops', 'commas', 'semicolon', 'colon',\n",
       "       'ellipsis', 'pronouns1st', 'pronouns2nd', 'pronouns3rd',\n",
       "       'compsupadjadv', 'past', 'imperative', 'present3rd', 'present1st2nd',\n",
       "       'sentence1st', 'sentencelast', 'txtcomplexity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maximum and minimum normalization\n",
    "def MaxMinNormalization(x):\n",
    "    x = (x - np.min(x.values)) / (np.max(x.values) - np.min(x.values))\n",
    "    return x\n",
    " \n",
    "for c in features:\n",
    "    data[c]=MaxMinNormalization(data[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TextID</th>\n",
       "      <th>URL</th>\n",
       "      <th>Label</th>\n",
       "      <th>totalWordsCount</th>\n",
       "      <th>semanticobjscore</th>\n",
       "      <th>semanticsubjscore</th>\n",
       "      <th>CC</th>\n",
       "      <th>CD</th>\n",
       "      <th>DT</th>\n",
       "      <th>EX</th>\n",
       "      <th>...</th>\n",
       "      <th>pronouns2nd</th>\n",
       "      <th>pronouns3rd</th>\n",
       "      <th>compsupadjadv</th>\n",
       "      <th>past</th>\n",
       "      <th>imperative</th>\n",
       "      <th>present3rd</th>\n",
       "      <th>present1st2nd</th>\n",
       "      <th>sentence1st</th>\n",
       "      <th>sentencelast</th>\n",
       "      <th>txtcomplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Text0001</td>\n",
       "      <td>http://msn.foxsports.com/foxsoccer/mexico/stor...</td>\n",
       "      <td>objective</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.031111</td>\n",
       "      <td>0.019108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023622</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.191176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Text0002</td>\n",
       "      <td>http://msn.foxsports.com/foxsoccer/premierleag...</td>\n",
       "      <td>objective</td>\n",
       "      <td>309</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>0.004444</td>\n",
       "      <td>0.040340</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078740</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098592</td>\n",
       "      <td>0.092784</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.132353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Text0003</td>\n",
       "      <td>http://uk.eurosport.yahoo.com/04022011/58/fed-...</td>\n",
       "      <td>objective</td>\n",
       "      <td>149</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.035556</td>\n",
       "      <td>0.029724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021127</td>\n",
       "      <td>0.020619</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.191176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Text0004</td>\n",
       "      <td>http://uk.eurosport.yahoo.com/07022011/58/bund...</td>\n",
       "      <td>objective</td>\n",
       "      <td>305</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>0.031111</td>\n",
       "      <td>0.055202</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062992</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.059091</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.049296</td>\n",
       "      <td>0.010309</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.220588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Text0005</td>\n",
       "      <td>http://uk.eurosport.yahoo.com/05022011/58/fed-...</td>\n",
       "      <td>objective</td>\n",
       "      <td>491</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>0.146667</td>\n",
       "      <td>0.099788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125984</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.154545</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.035211</td>\n",
       "      <td>0.061856</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.279412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Text0996</td>\n",
       "      <td>http://www.nba.com/pistons/news/blaha_090313.html</td>\n",
       "      <td>subjective</td>\n",
       "      <td>926</td>\n",
       "      <td>46</td>\n",
       "      <td>34</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.176221</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.362205</td>\n",
       "      <td>0.256410</td>\n",
       "      <td>0.059091</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.345070</td>\n",
       "      <td>0.298969</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.279412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Text0997</td>\n",
       "      <td>http://www.nba.com/pistons/news/smashing_succe...</td>\n",
       "      <td>subjective</td>\n",
       "      <td>876</td>\n",
       "      <td>48</td>\n",
       "      <td>26</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.231423</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.338583</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>0.154545</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.133803</td>\n",
       "      <td>0.319588</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Text0998</td>\n",
       "      <td>http://www.foxsportswest.com/01/07/13/Hypocrit...</td>\n",
       "      <td>subjective</td>\n",
       "      <td>1469</td>\n",
       "      <td>82</td>\n",
       "      <td>53</td>\n",
       "      <td>0.062222</td>\n",
       "      <td>0.363057</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204545</td>\n",
       "      <td>0.385827</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.359155</td>\n",
       "      <td>0.432990</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.191176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Text0999</td>\n",
       "      <td>http://www.football365.com/f365-features/84318...</td>\n",
       "      <td>subjective</td>\n",
       "      <td>343</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.017778</td>\n",
       "      <td>0.050955</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062992</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.013636</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.176056</td>\n",
       "      <td>0.072165</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.088235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Text1000</td>\n",
       "      <td>http://www.nba.com/pistons/features/blaha_1103...</td>\n",
       "      <td>subjective</td>\n",
       "      <td>823</td>\n",
       "      <td>43</td>\n",
       "      <td>22</td>\n",
       "      <td>0.084444</td>\n",
       "      <td>0.212314</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204545</td>\n",
       "      <td>0.141732</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.204545</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.112676</td>\n",
       "      <td>0.164948</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.264706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       TextID                                                URL       Label  \\\n",
       "0    Text0001  http://msn.foxsports.com/foxsoccer/mexico/stor...   objective   \n",
       "1    Text0002  http://msn.foxsports.com/foxsoccer/premierleag...   objective   \n",
       "2    Text0003  http://uk.eurosport.yahoo.com/04022011/58/fed-...   objective   \n",
       "3    Text0004  http://uk.eurosport.yahoo.com/07022011/58/bund...   objective   \n",
       "4    Text0005  http://uk.eurosport.yahoo.com/05022011/58/fed-...   objective   \n",
       "..        ...                                                ...         ...   \n",
       "995  Text0996  http://www.nba.com/pistons/news/blaha_090313.html  subjective   \n",
       "996  Text0997  http://www.nba.com/pistons/news/smashing_succe...  subjective   \n",
       "997  Text0998  http://www.foxsportswest.com/01/07/13/Hypocrit...  subjective   \n",
       "998  Text0999  http://www.football365.com/f365-features/84318...  subjective   \n",
       "999  Text1000  http://www.nba.com/pistons/features/blaha_1103...  subjective   \n",
       "\n",
       "     totalWordsCount  semanticobjscore  semanticsubjscore        CC        CD  \\\n",
       "0                109                 0                  1  0.031111  0.019108   \n",
       "1                309                21                  4  0.004444  0.040340   \n",
       "2                149                 6                  1  0.035556  0.029724   \n",
       "3                305                18                  5  0.031111  0.055202   \n",
       "4                491                23                  8  0.146667  0.099788   \n",
       "..               ...               ...                ...       ...       ...   \n",
       "995              926                46                 34  0.022222  0.176221   \n",
       "996              876                48                 26  0.040000  0.231423   \n",
       "997             1469                82                 53  0.062222  0.363057   \n",
       "998              343                 7                  5  0.017778  0.050955   \n",
       "999              823                43                 22  0.084444  0.212314   \n",
       "\n",
       "           DT        EX  ...  pronouns2nd  pronouns3rd  compsupadjadv  \\\n",
       "0    0.000000  0.052083  ...     0.000000     0.023622       0.000000   \n",
       "1    0.083333  0.041667  ...     0.000000     0.078740       0.000000   \n",
       "2    0.000000  0.052083  ...     0.000000     0.015748       0.000000   \n",
       "3    0.000000  0.104167  ...     0.000000     0.062992       0.076923   \n",
       "4    0.000000  0.125000  ...     0.000000     0.125984       0.051282   \n",
       "..        ...       ...  ...          ...          ...            ...   \n",
       "995  0.083333  0.208333  ...     0.363636     0.362205       0.256410   \n",
       "996  0.083333  0.166667  ...     0.181818     0.338583       0.128205   \n",
       "997  0.083333  0.104167  ...     0.204545     0.385827       0.307692   \n",
       "998  0.000000  0.093750  ...     0.000000     0.062992       0.025641   \n",
       "999  0.083333  0.135417  ...     0.204545     0.141732       0.153846   \n",
       "\n",
       "         past  imperative  present3rd  present1st2nd  sentence1st  \\\n",
       "0    0.050000    0.000000    0.000000       0.000000          0.0   \n",
       "1    0.059091    0.000000    0.098592       0.092784          1.0   \n",
       "2    0.036364    0.000000    0.021127       0.020619          1.0   \n",
       "3    0.059091    0.032258    0.049296       0.010309          1.0   \n",
       "4    0.154545    0.032258    0.035211       0.061856          1.0   \n",
       "..        ...         ...         ...            ...          ...   \n",
       "995  0.059091    0.387097    0.345070       0.298969          1.0   \n",
       "996  0.154545    0.290323    0.133803       0.319588          1.0   \n",
       "997  0.181818    0.612903    0.359155       0.432990          1.0   \n",
       "998  0.013636    0.096774    0.176056       0.072165          1.0   \n",
       "999  0.204545    0.225806    0.112676       0.164948          1.0   \n",
       "\n",
       "     sentencelast  txtcomplexity  \n",
       "0             1.0       0.191176  \n",
       "1             1.0       0.132353  \n",
       "2             1.0       0.191176  \n",
       "3             1.0       0.220588  \n",
       "4             1.0       0.279412  \n",
       "..            ...            ...  \n",
       "995           1.0       0.279412  \n",
       "996           1.0       0.235294  \n",
       "997           1.0       0.191176  \n",
       "998           1.0       0.088235  \n",
       "999           1.0       0.264706  \n",
       "\n",
       "[1000 rows x 62 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "objective     635\n",
       "subjective    365\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Label'].replace((\"objective\",\"subjective\"),(0,1),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1000 entries, 915 to 810\n",
      "Data columns (total 62 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   TextID             1000 non-null   object \n",
      " 1   URL                1000 non-null   object \n",
      " 2   Label              1000 non-null   int64  \n",
      " 3   totalWordsCount    1000 non-null   int64  \n",
      " 4   semanticobjscore   1000 non-null   int64  \n",
      " 5   semanticsubjscore  1000 non-null   int64  \n",
      " 6   CC                 1000 non-null   float64\n",
      " 7   CD                 1000 non-null   float64\n",
      " 8   DT                 1000 non-null   float64\n",
      " 9   EX                 1000 non-null   float64\n",
      " 10  FW                 1000 non-null   float64\n",
      " 11  INs                1000 non-null   float64\n",
      " 12  JJ                 1000 non-null   float64\n",
      " 13  JJR                1000 non-null   float64\n",
      " 14  JJS                1000 non-null   float64\n",
      " 15  LS                 1000 non-null   float64\n",
      " 16  MD                 1000 non-null   float64\n",
      " 17  NN                 1000 non-null   float64\n",
      " 18  NNP                1000 non-null   int64  \n",
      " 19  NNPS               1000 non-null   float64\n",
      " 20  NNS                1000 non-null   float64\n",
      " 21  PDT                1000 non-null   float64\n",
      " 22  POS                1000 non-null   float64\n",
      " 23  PRP                1000 non-null   float64\n",
      " 24  PRP$               1000 non-null   float64\n",
      " 25  RB                 1000 non-null   float64\n",
      " 26  RBR                1000 non-null   float64\n",
      " 27  RBS                1000 non-null   float64\n",
      " 28  RP                 1000 non-null   float64\n",
      " 29  SYM                1000 non-null   float64\n",
      " 30  TOs                1000 non-null   float64\n",
      " 31  UH                 1000 non-null   float64\n",
      " 32  VB                 1000 non-null   float64\n",
      " 33  VBD                1000 non-null   float64\n",
      " 34  VBG                1000 non-null   float64\n",
      " 35  VBN                1000 non-null   float64\n",
      " 36  VBP                1000 non-null   float64\n",
      " 37  VBZ                1000 non-null   float64\n",
      " 38  WDT                1000 non-null   float64\n",
      " 39  WP                 1000 non-null   float64\n",
      " 40  WP$                1000 non-null   float64\n",
      " 41  WRB                1000 non-null   int64  \n",
      " 42  baseform           1000 non-null   float64\n",
      " 43  Quotes             1000 non-null   float64\n",
      " 44  questionmarks      1000 non-null   float64\n",
      " 45  exclamationmarks   1000 non-null   float64\n",
      " 46  fullstops          1000 non-null   float64\n",
      " 47  commas             1000 non-null   float64\n",
      " 48  semicolon          1000 non-null   float64\n",
      " 49  colon              1000 non-null   float64\n",
      " 50  ellipsis           1000 non-null   float64\n",
      " 51  pronouns1st        1000 non-null   float64\n",
      " 52  pronouns2nd        1000 non-null   float64\n",
      " 53  pronouns3rd        1000 non-null   float64\n",
      " 54  compsupadjadv      1000 non-null   float64\n",
      " 55  past               1000 non-null   float64\n",
      " 56  imperative         1000 non-null   float64\n",
      " 57  present3rd         1000 non-null   float64\n",
      " 58  present1st2nd      1000 non-null   float64\n",
      " 59  sentence1st        1000 non-null   float64\n",
      " 60  sentencelast       1000 non-null   float64\n",
      " 61  txtcomplexity      1000 non-null   float64\n",
      "dtypes: float64(54), int64(6), object(2)\n",
      "memory usage: 492.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "    BP neural network model. The weights between layers are randomly generated and obey the standard normal distribution.\n",
    "\"\"\"\n",
    "\n",
    "def Sigmoid(x):\n",
    "    \"\"\"\n",
    "    calculation formula of the S-type activation function.\n",
    "    :param x: Data to be calculated.\n",
    "    :return: Function value of S-type activation function.\n",
    "    \"\"\"\n",
    "    function = 1.0 / (1.0 + np.exp(-x))\n",
    "    return function\n",
    "\n",
    "def Sigmoid_Derivative(x):\n",
    "    \"\"\"\n",
    "    derivative calculation formula of the S-type activation function.\n",
    "    \n",
    "    \"\"\"\n",
    "    f = Sigmoid(x)\n",
    "    derivative = f*(1.0-f)\n",
    "    return derivative\n",
    "\n",
    "class BPNN:\n",
    "    def __init__(self,input_n,hidden_n,output_n,input_weights = None,hidden_weights = None,\n",
    "                 hidden_threshold = None,output_threshold = None):\n",
    "        \"\"\"\n",
    "        constructor of the BP neural network class.\n",
    "        :param input_n: Number of neurons in the input layer.\n",
    "        :param hidden_n: Number of hidden layer neurons.\n",
    "        :param output_n: Number of neurons in the output layer.\n",
    "        \"\"\"\n",
    "        self.Train_Data = []                                                                        # 训练数据集\n",
    "        self.Train_Label = []                                                                       # 训练数据集标签\n",
    "        self.input_n = input_n                                                                      # 输入层神经元个数\n",
    "        self.hidden_n = hidden_n                                                                    # 隐含层神经元个数\n",
    "        self.output_n = output_n                                                                    # 输出层神经元个数\n",
    "        self.input_cells = np.zeros(self.input_n).reshape((1,self.input_n))                         # 输入层神经元\n",
    "        self.hidden_cells = np.zeros(self.hidden_n).reshape((1,self.hidden_n))                      # 隐含层神经元\n",
    "        self.hidden_cells_input = np.zeros(self.hidden_n).reshape((1,self.hidden_n))                # 隐含层的输入（不含阈值后进行sigmoid）\n",
    "        self.output_cells = np.zeros(self.output_n).reshape((1,self.output_n))                      # 输出层神经元\n",
    "        self.output_cells_input = np.zeros(self.output_n).reshape((1,self.output_n))                # 输出层的输入（不含阈值后进行sigmoid）\n",
    "        if input_weights is not None:\n",
    "            self.input_weights = input_weights\n",
    "        else:\n",
    "            self.input_weights = np.random.randn(self.input_n,self.hidden_n)                        # 输入层与隐含层之间的权重\n",
    "        if hidden_weights is not None:\n",
    "            self.hidden_weights = hidden_weights\n",
    "        else:\n",
    "            self.hidden_weights = np.random.randn(self.hidden_n,self.output_n)                      # 隐含层与输出层之间的权重\n",
    "        if hidden_threshold is not None:\n",
    "            self.hidden_threshold = hidden_threshold\n",
    "        else:\n",
    "            self.hidden_threshold = np.random.randn(1, self.hidden_n)                               # 隐含层的阈值\n",
    "        if output_threshold is not None:\n",
    "            self.output_threshold = output_threshold\n",
    "        else:\n",
    "            self.output_threshold = np.random.randn(1, self.output_n)                               # 输出层的阈值\n",
    "        self.input_weights_copy = deepcopy(self.input_weights)                                      # 输入层与隐含层之间的权重备份\n",
    "        self.hidden_weights_copy = deepcopy(self.hidden_weights)                                    # 隐含层与输出层之间的权重备份\n",
    "        self.hidden_threshold_copy = deepcopy(self.hidden_threshold)                                # 隐含层的阈值备份\n",
    "        self.output_threshold_copy = deepcopy(self.output_threshold)                                # 输出层的阈值备份\n",
    "\n",
    "    def Init(self,Train_Data,Train_Label):\n",
    "        \"\"\"\n",
    "        Function to initialize training data and labels\n",
    "        \"\"\"\n",
    "        self.Train_Data = Train_Data\n",
    "        self.Train_Label = Train_Label\n",
    "\n",
    "    def predict(self,input):\n",
    "        \"\"\"\n",
    "        BP neural network forward learning transfer function\n",
    "        \"\"\"\n",
    "        # Copy input layer input data\n",
    "        self.input_cells = deepcopy(input)\n",
    "        # Calculate hidden layer output\n",
    "        self.hidden_cells_input = self.input_cells.dot(self.input_weights)\n",
    "        self.hidden_cells = self.hidden_cells_input + self.hidden_threshold\n",
    "        self.hidden_cells = Sigmoid(self.hidden_cells)\n",
    "        # Calculate output layer output\n",
    "        self.output_cells_input = self.hidden_cells.dot(self.hidden_weights)\n",
    "        self.output_cells = self.output_cells_input + self.output_threshold\n",
    "        self.output_cells = Sigmoid(self.output_cells)\n",
    "        return self.output_cells\n",
    "\n",
    "    def Gradient_Vector(self,ideal_output):\n",
    "        \"\"\"\n",
    "        Function to calculate the relevant gradient vector of each set of input data\n",
    "        :param ideal_output: target or actual real output\n",
    "        \"\"\"\n",
    "        # The updated relative gradient of the weight and threshold between the hidden layer and the output layer\n",
    "        error = ideal_output - self.output_cells\n",
    "        derivative = Sigmoid_Derivative(self.output_cells)\n",
    "        g = derivative * error\n",
    "        # Hidden layer-output layer weight increment\n",
    "        hidden_weights_increasement = self.hidden_cells.T.dot(g)\n",
    "        # Hidden layer threshold increment\n",
    "        output_threshold_increasement = -g\n",
    "        # \n",
    "        e = Sigmoid_Derivative(self.hidden_cells) * g.dot(self.hidden_weights.T)\n",
    "        # Input layer-hidden layer weight increment\n",
    "        input_weights_increasement = self.input_cells.T.dot(e)\n",
    "        # Input layer threshold increment\n",
    "        hidden_threshold_increasement = -e\n",
    "        return hidden_weights_increasement,output_threshold_increasement\\\n",
    "            ,input_weights_increasement,hidden_threshold_increasement\n",
    "\n",
    "    def back_propagate(self,hidden_weights_increasement,output_threshold_increasement\\\n",
    "            ,input_weights_increasement,hidden_threshold_increasement,learn_rate):\n",
    "        \"\"\"\n",
    "        Backward error transfer function, parameter adjustment\n",
    "        :param g: The updated relative gradient of the weight and threshold between the hidden layer and the output layer\n",
    "        :param e: The updated relative gradient of the weight and threshold between the input layer and the output layer\n",
    "        :param input_cell: The sum of the input layer\n",
    "        :param hidden_cell: The sum of hidden layers\n",
    "        \"\"\"\n",
    "        # Update of the weight between the hidden layer and the output layer\n",
    "        self.hidden_weights = self.hidden_weights + hidden_weights_increasement*learn_rate\n",
    "        #Threshold update of the output layer\n",
    "        self.output_threshold = self.output_threshold + output_threshold_increasement*learn_rate\n",
    "        #Weight update between input layer and hidden layer\n",
    "        self.input_weights = self.input_weights + learn_rate*input_weights_increasement\n",
    "        #Hidden layer threshold update\n",
    "        self.hidden_threshold = self.hidden_threshold + hidden_threshold_increasement*learn_rate\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        reset function of BPNN, which restores the initial values of related parameters after changing an iteration process\n",
    "        \"\"\"\n",
    "        self.input_weights = deepcopy(self.input_weights_copy)  # Weight backup between input layer and hidden layer\n",
    "        self.hidden_weights = deepcopy(self.hidden_weights_copy)  # Weight backup between hidden layer and output layer\n",
    "        self.hidden_threshold = deepcopy(self.hidden_threshold_copy)  # Threshold backup of hidden layer\n",
    "        self.output_threshold = deepcopy(self.output_threshold_copy)  # Threshold backup of the output layer\n",
    "\n",
    "    def train_batch(self,input,output):\n",
    "        \"\"\"\n",
    "        Function to train on one set of data at a time\n",
    "        \n",
    "        \"\"\"\n",
    "        # predicted classification result\n",
    "        \n",
    "        output = output.reshape(1, len(output))\n",
    "        input = input.reshape(1, len(input))\n",
    "        self.output_cells = self.predict(input)\n",
    "        # Calculate the relevant gradient vector\n",
    "        hidden_weights_increasement, output_threshold_increasement \\\n",
    "            , input_weights_increasement, hidden_threshold_increasement = self.Gradient_Vector(output)\n",
    "        # Calculate classification error\n",
    "        error = np.sum((output-self.output_cells)**2)\n",
    "        return hidden_weights_increasement,output_threshold_increasement\\\n",
    "            ,input_weights_increasement,hidden_threshold_increasement,error\n",
    "\n",
    "    def train_BGD(self,inputs,outputs,limitation,learn_rate):\n",
    "        \"\"\"\n",
    "        BP neural network training function, using BGD algorithm for parameter update\n",
    "        \n",
    "        :param limitaion: Number of iterations\n",
    "        \n",
    "        \"\"\"\n",
    "        #Initialize the training data set and the corresponding label\n",
    "        self.Init(inputs,outputs)\n",
    "        for j in range(limitation):\n",
    "            self.train_dataset_BGD(learn_rate)\n",
    "\n",
    "    def train_dataset_BGD(self,learn_rate):\n",
    "        \"\"\"\n",
    "        The function of using the BGD algorithm to train the entire training set in one iteration\n",
    "        \n",
    "        \"\"\"\n",
    "        Hidden_Weights_Increasement = []                                # Hidden layer-weight increment between output layer\n",
    "        Output_Threshold_Increasement = []                              # Threshold increment between output layers\n",
    "        Input_Weights_Increasement = []                                 # Weight increment between input layer and hidden layer\n",
    "        Hidden_Threshold_Increasement = []                              # Hidden layer threshold increment\n",
    "        for (train_data,train_label) in zip(self.Train_Data,self.Train_Label):\n",
    "            # Traverse and train, and get the relevant gradient increment and the corresponding error\n",
    "            hidden_weights_increasement, output_threshold_increasement \\\n",
    "            ,input_weights_increasement, hidden_threshold_increasement,err = self.train_batch(train_data,train_label)\n",
    "            Hidden_Weights_Increasement.append(hidden_weights_increasement)\n",
    "            Output_Threshold_Increasement.append(output_threshold_increasement)\n",
    "            Input_Weights_Increasement.append(input_weights_increasement)\n",
    "            Hidden_Threshold_Increasement.append(hidden_threshold_increasement)\n",
    "        # Calculate the total gradient\n",
    "        hidden_weights_increasement_sum = np.sum(np.array(Hidden_Weights_Increasement),0)\n",
    "        output_threshold_increasement_sum = np.sum(np.array(Output_Threshold_Increasement),0)\n",
    "        input_weights_increasement_sum = np.sum(np.array(Input_Weights_Increasement),0)\n",
    "        hidden_threshold_increasement_sum = np.sum(np.array(Hidden_Threshold_Increasement),0)\n",
    "        # Perform error back propagation and adjust hyperparameters\n",
    "        self.back_propagate(hidden_weights_increasement_sum,output_threshold_increasement_sum,\n",
    "                            input_weights_increasement_sum,hidden_threshold_increasement_sum,learn_rate)\n",
    "\n",
    "    def train_SGD(self,inputs,outputs,limitation,learn_rate):\n",
    "        \"\"\"\n",
    "        Training function of BP neural network, using SGD algorithm to update parameters\n",
    "        \n",
    "        \"\"\"\n",
    "        #Initialize the training data set and the corresponding label\n",
    "        self.Init(inputs,outputs)\n",
    "        for j in range(limitation):\n",
    "            self.train_dataset_SGD(learn_rate)\n",
    "\n",
    "    def train_dataset_SGD(self, learn_rate):\n",
    "        \"\"\"\n",
    "        A function to train the entire training set using the SGD algorithm in one iteration\n",
    "        \n",
    "        \"\"\"\n",
    "        # Randomly shuffle the data set and label set\n",
    "        self.Shuffle_SGD()\n",
    "        for (train_data, train_label) in zip(self.Train_Data, self.Train_Label):\n",
    "            # \n",
    "            hidden_weights_increasement, output_threshold_increasement, input_weights_increasement, \\\n",
    "                    hidden_threshold_increasement, err = self.train_batch(train_data,train_label)\n",
    "            # Perform error back propagation and adjust hyperparameters\n",
    "            self.back_propagate(hidden_weights_increasement, output_threshold_increasement,\n",
    "                                input_weights_increasement, hidden_threshold_increasement, learn_rate)\n",
    "\n",
    "    def Shuffle_SGD(self):\n",
    "        \"\"\"\n",
    "        Function to randomly shuffle the data set and the mark set before executing the SGD algorithm\n",
    "        \"\"\"\n",
    "        #First generate a sequence of natural numbers with the same length as the training data for the SGD algorithm and scramble\n",
    "        length = len(self.Train_Label)\n",
    "        random_sequence = list(np.arange(length))\n",
    "        random.shuffle(random_sequence)\n",
    "        data = [self.Train_Data[index] for index in random_sequence]\n",
    "        label = [self.Train_Label[index] for index in random_sequence]\n",
    "        self.Train_Data = np.array(data)\n",
    "        self.Train_Label = np.array(label)\n",
    "\n",
    "    def train_MBGD(self,inputs,outputs,limitation,learn_rate,mini_batch_size):\n",
    "        \"\"\"\n",
    "        using MBGD algorithm for parameter update\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.Init(inputs,outputs)\n",
    "        for j in range(limitation):\n",
    "            self.train_dataset_MBGD(learn_rate,mini_batch_size)\n",
    "\n",
    "    def train_dataset_MBGD(self, learn_rate,mini_batch_size):\n",
    "        \"\"\"\n",
    "        A function to train the entire training set using the MBGD algorithm in one iteration\n",
    "        \"\"\"\n",
    "        \n",
    "        self.Split_MBGD(mini_batch_size)\n",
    "        for (mini_batch_data, mini_batch_label) in zip(self.Train_Data, self.Train_Label):\n",
    "            Hidden_Weights_Increasement = []  # Hidden layer-weight increment between output layer\n",
    "            Output_Threshold_Increasement = []  # Threshold increment between output layers\n",
    "            Input_Weights_Increasement = []  # Weight increment between input layer and hidden layer\n",
    "            Hidden_Threshold_Increasement = []  # Hidden layer threshold increment\n",
    "            # Traverse and train, and get the relevant gradient increment and the corresponding error\n",
    "            for (train_data,train_label) in zip(mini_batch_data, mini_batch_label):\n",
    "                hidden_weights_increasement, output_threshold_increasement, input_weights_increasement, \\\n",
    "                        hidden_threshold_increasement, err = self.train_batch(train_data,train_label)\n",
    "                Hidden_Weights_Increasement.append(hidden_weights_increasement)\n",
    "                Output_Threshold_Increasement.append(output_threshold_increasement)\n",
    "                Input_Weights_Increasement.append(input_weights_increasement)\n",
    "                Hidden_Threshold_Increasement.append(hidden_threshold_increasement)\n",
    "            # Calculate the gradient average\n",
    "            hidden_weights_increasement_avg = np.average(np.array(Hidden_Weights_Increasement), 0)\n",
    "            output_threshold_increasement_avg = np.average(np.array(Output_Threshold_Increasement), 0)\n",
    "            input_weights_increasement_avg = np.average(np.array(Input_Weights_Increasement), 0)\n",
    "            hidden_threshold_increasement_avg = np.average(np.array(Hidden_Threshold_Increasement), 0)\n",
    "            # Perform error back propagation and adjust hyperparameters\n",
    "            self.back_propagate(hidden_weights_increasement_avg, output_threshold_increasement_avg,\n",
    "                                input_weights_increasement_avg, hidden_threshold_increasement_avg, learn_rate)\n",
    "        self.Return()\n",
    "\n",
    "    def Split_MBGD(self,mini_batch_size):\n",
    "        \"\"\"\n",
    "        Before executing the MBGD algorithm, a function to generate mini-batches from the data set and the marker set\n",
    "        \"\"\"\n",
    "        '''\n",
    "        Since it is not known whether the length of the data set can be divisible by mini_batch_size,\n",
    "        First calculate the remainder rest of the data set length and mini_batch_size,\n",
    "        If rest is zero, it is better not to execute the following if judgment statement,\n",
    "        If it is not zero, first allocate the data set without the remainder,\n",
    "        Then add the remaining data and results through the following if statement.\n",
    "       '''\n",
    "        length = len(self.Train_Label)\n",
    "        rest = length % mini_batch_size\n",
    "        random_sequence = np.arange(length)\n",
    "        random.shuffle(random_sequence)\n",
    "        mini_batches = [random_sequence[k:k + mini_batch_size] for k in\n",
    "                        range(0, length - rest, mini_batch_size)]\n",
    "        mini_batch_inputs = []\n",
    "        mini_batch_outputs = []\n",
    "        for batch in mini_batches:\n",
    "            mini_batch_data = [self.Train_Data[_batch] for _batch in batch]\n",
    "            mini_batch_label = [self.Train_Label[_batch] for _batch in batch]\n",
    "            mini_batch_inputs.append(list(mini_batch_data))\n",
    "            mini_batch_outputs.append(list(mini_batch_label))\n",
    "        # The length of the data set cannot be divisible by mini_batch_size, and the remaining data forms the last mini_batch\n",
    "        if rest > 0:\n",
    "            batch = random_sequence[length - rest:length]\n",
    "            mini_batch_data = [self.Train_Data[_batch] for _batch in batch]\n",
    "            mini_batch_label = [self.Train_Label[_batch] for _batch in batch]\n",
    "            mini_batch_inputs.append(list(mini_batch_data))\n",
    "            mini_batch_outputs.append(list(mini_batch_label))\n",
    "        self.Train_Data = np.array(mini_batch_inputs)\n",
    "        self.Train_Label = np.array(mini_batch_outputs)\n",
    "\n",
    "    def Return(self):\n",
    "        \"\"\"\n",
    "        The function to restore the small sample data set and label set of the MBGD algorithm\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        label = []\n",
    "        for (train_data,train_label) in zip(self.Train_Data,self.Train_Label):\n",
    "            for (_data,_label) in zip(train_data,train_label):\n",
    "                data.append(_data)\n",
    "                label.append(_label)\n",
    "        self.Train_Data = np.array(data)\n",
    "        self.Train_Label = np.array(label)\n",
    "\n",
    "    def test(self,Test_Data):\n",
    "        \"\"\"\n",
    "        BP neural network test function\n",
    "        \"\"\"\n",
    "        predict_labels = []\n",
    "        # Generate standard output neurons and set 0\n",
    "        tmp = [0]*self.output_n\n",
    "        for test_data in Test_Data:\n",
    "            predict_output = self.predict(test_data)\n",
    "            index = np.argmax(predict_output)\n",
    "            tmp1 = deepcopy(tmp)\n",
    "            tmp1[index] = 1\n",
    "            predict_labels.append(tmp1)\n",
    "        predict_labels = np.array(predict_labels)\n",
    "        return predict_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Merge(data,columns):\n",
    "    \"\"\"\n",
    "    This is the function to merge arrays into DataFrame\n",
    "    \n",
    "    \"\"\"\n",
    "    Data = np.array(data)\n",
    "    Data = Data.T\n",
    "    ans = pd.DataFrame(data=Data,columns=columns)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=np.array(data[features])\n",
    "Label=[]\n",
    "for i in data[\"Label\"].values:\n",
    "    if i==0:\n",
    "        Label.append([1.0,0.0])\n",
    "    else:\n",
    "        Label.append([0.0,1.0])\n",
    "Label=np.array(Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01777778, 0.17622081, 0.41666667, ..., 1.        , 1.        ,\n",
       "        0.16176471],\n",
       "       [0.19555556, 0.21019108, 0.08333333, ..., 1.        , 1.        ,\n",
       "        0.19117647],\n",
       "       [0.10666667, 0.16135881, 0.        , ..., 1.        , 1.        ,\n",
       "        0.19117647],\n",
       "       ...,\n",
       "       [0.06222222, 0.15498938, 0.08333333, ..., 1.        , 1.        ,\n",
       "        0.25      ],\n",
       "       [0.07111111, 0.16985138, 0.16666667, ..., 1.        , 1.        ,\n",
       "        0.16176471],\n",
       "       [0.16      , 0.1910828 , 0.08333333, ..., 1.        , 1.        ,\n",
       "        0.19117647]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-d5ef9f9e78b7>:299: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  self.Train_Data = np.array(mini_batch_inputs)\n",
      "<ipython-input-17-d5ef9f9e78b7>:300: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  self.Train_Label = np.array(mini_batch_outputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10,learn_rate=0.010000\n",
      "BGD TA: 0.660000,MSE：0.680000\n",
      "SGD TA: 0.650000,MSE：0.700000\n",
      "MBGD TA：0.660000,MSE：0.680000\n",
      "20,learn_rate=0.010000\n",
      "BGD TA: 0.795000,MSE：0.410000\n",
      "SGD TA: 0.755000,MSE：0.490000\n",
      "MBGD TA：0.795000,MSE：0.410000\n",
      "30,learn_rate=0.010000\n",
      "BGD TA: 0.815000,MSE：0.370000\n",
      "SGD TA: 0.795000,MSE：0.410000\n",
      "MBGD TA：0.815000,MSE：0.370000\n",
      "40,learn_rate=0.010000\n",
      "BGD TA: 0.825000,MSE：0.350000\n",
      "SGD TA: 0.810000,MSE：0.380000\n",
      "MBGD TA：0.825000,MSE：0.350000\n",
      "50,learn_rate=0.010000\n",
      "BGD TA: 0.820000,MSE：0.360000\n",
      "SGD TA: 0.810000,MSE：0.380000\n",
      "MBGD TA：0.820000,MSE：0.360000\n",
      "60,learn_rate=0.010000\n",
      "BGD TA: 0.825000,MSE：0.350000\n",
      "SGD TA: 0.810000,MSE：0.380000\n",
      "MBGD TA：0.825000,MSE：0.350000\n",
      "70,learn_rate=0.010000\n",
      "BGD TA: 0.825000,MSE：0.350000\n",
      "SGD TA: 0.810000,MSE：0.380000\n",
      "MBGD TA：0.825000,MSE：0.350000\n",
      "80,learn_rate=0.010000\n",
      "BGD TA: 0.825000,MSE：0.350000\n",
      "SGD TA: 0.820000,MSE：0.360000\n",
      "MBGD TA：0.825000,MSE：0.350000\n",
      "90,learn_rate=0.010000\n",
      "BGD TA: 0.825000,MSE：0.350000\n",
      "SGD TA: 0.825000,MSE：0.350000\n",
      "MBGD TA：0.825000,MSE：0.350000\n",
      "100,learn_rate=0.010000\n",
      "BGD TA: 0.825000,MSE：0.350000\n",
      "SGD TA: 0.825000,MSE：0.350000\n",
      "MBGD TA：0.825000,MSE：0.350000\n",
      "110,learn_rate=0.010000\n",
      "BGD TA: 0.825000,MSE：0.350000\n",
      "SGD TA: 0.830000,MSE：0.340000\n",
      "MBGD TA：0.825000,MSE：0.350000\n",
      "120,learn_rate=0.010000\n",
      "BGD TA: 0.825000,MSE：0.350000\n",
      "SGD TA: 0.830000,MSE：0.340000\n",
      "MBGD TA：0.825000,MSE：0.350000\n",
      "130,learn_rate=0.010000\n",
      "BGD TA: 0.835000,MSE：0.330000\n",
      "SGD TA: 0.835000,MSE：0.330000\n",
      "MBGD TA：0.835000,MSE：0.330000\n",
      "140,learn_rate=0.010000\n",
      "BGD TA: 0.835000,MSE：0.330000\n",
      "SGD TA: 0.835000,MSE：0.330000\n",
      "MBGD TA：0.835000,MSE：0.330000\n",
      "150,learn_rate=0.010000\n",
      "BGD TA: 0.835000,MSE：0.330000\n",
      "SGD TA: 0.835000,MSE：0.330000\n",
      "MBGD TA：0.835000,MSE：0.330000\n",
      "160,learn_rate=0.010000\n",
      "BGD TA: 0.835000,MSE：0.330000\n",
      "SGD TA: 0.835000,MSE：0.330000\n",
      "MBGD TA：0.835000,MSE：0.330000\n",
      "170,learn_rate=0.010000\n",
      "BGD TA: 0.835000,MSE：0.330000\n",
      "SGD TA: 0.835000,MSE：0.330000\n",
      "MBGD TA：0.835000,MSE：0.330000\n",
      "180,learn_rate=0.010000\n",
      "BGD TA: 0.840000,MSE：0.320000\n",
      "SGD TA: 0.835000,MSE：0.330000\n",
      "MBGD TA：0.840000,MSE：0.320000\n",
      "190,learn_rate=0.010000\n",
      "BGD TA: 0.835000,MSE：0.330000\n",
      "SGD TA: 0.840000,MSE：0.320000\n",
      "MBGD TA：0.835000,MSE：0.330000\n",
      "200,learn_rate=0.010000\n",
      "BGD TA: 0.835000,MSE：0.330000\n",
      "SGD TA: 0.835000,MSE：0.330000\n",
      "MBGD TA：0.835000,MSE：0.330000\n",
      "210,learn_rate=0.010000\n",
      "BGD TA: 0.835000,MSE：0.330000\n",
      "SGD TA: 0.840000,MSE：0.320000\n",
      "MBGD TA：0.835000,MSE：0.330000\n",
      "220,learn_rate=0.010000\n",
      "BGD TA: 0.835000,MSE：0.330000\n",
      "SGD TA: 0.840000,MSE：0.320000\n",
      "MBGD TA：0.835000,MSE：0.330000\n",
      "230,learn_rate=0.010000\n",
      "BGD TA: 0.830000,MSE：0.340000\n",
      "SGD TA: 0.835000,MSE：0.330000\n",
      "MBGD TA：0.830000,MSE：0.340000\n",
      "240,learn_rate=0.010000\n",
      "BGD TA: 0.825000,MSE：0.350000\n",
      "SGD TA: 0.835000,MSE：0.330000\n",
      "MBGD TA：0.825000,MSE：0.350000\n",
      "250,learn_rate=0.010000\n",
      "BGD TA: 0.825000,MSE：0.350000\n",
      "SGD TA: 0.830000,MSE：0.340000\n",
      "MBGD TA：0.825000,MSE：0.350000\n",
      "260,learn_rate=0.010000\n",
      "BGD TA: 0.825000,MSE：0.350000\n",
      "SGD TA: 0.825000,MSE：0.350000\n",
      "MBGD TA：0.825000,MSE：0.350000\n",
      "270,learn_rate=0.010000\n",
      "BGD TA: 0.825000,MSE：0.350000\n",
      "SGD TA: 0.830000,MSE：0.340000\n",
      "MBGD TA：0.825000,MSE：0.350000\n",
      "280,learn_rate=0.010000\n",
      "BGD TA: 0.825000,MSE：0.350000\n",
      "SGD TA: 0.825000,MSE：0.350000\n",
      "MBGD TA：0.825000,MSE：0.350000\n",
      "290,learn_rate=0.010000\n",
      "BGD TA: 0.825000,MSE：0.350000\n",
      "SGD TA: 0.820000,MSE：0.360000\n",
      "MBGD TA：0.825000,MSE：0.350000\n"
     ]
    }
   ],
   "source": [
    "best_accr=0\n",
    "best_model=0\n",
    "mpl.rcParams['font.sans-serif'] = [u'simHei']\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# Initialize the array of iteration times, small sample size, L2 regularization parameter, learning rate array\n",
    "learn_rate = 0.01\n",
    "Iteration = 800\n",
    "mini_batch = 64\n",
    "\n",
    "# Initialize BP neural network and interstitial BP neural network and Brain_Interstitial_BPNN model\n",
    "col1 = np.shape(Data)[1]\n",
    "col2 = np.shape(Label)[1]\n",
    "input_n = col1\n",
    "hidden_n = int(np.sqrt(col1 * col2))\n",
    "output_n = col2\n",
    "\n",
    "# \n",
    "BGD_Accuracy = []               # Average accuracy of BGD algorithm\n",
    "SGD_Accuracy = []               # SGD algorithm average accuracy\n",
    "MBGD_Accuracy = []              # Average accuracy of MBGD algorithm\n",
    "BGD_MSE = []                    # BGD algorithm MSE\n",
    "SGD_MSE = []                    # SGD algorithm MSE\n",
    "MBGD_MSE = []                   # MBGD algorithm MSE\n",
    "\n",
    "# \n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=np.random.randint(0, len(Data)))\n",
    "train_test_index = []\n",
    "for (train_index, test_index) in kf.split(Data):\n",
    "    train_test_index.append((train_index, test_index))\n",
    "for (train_index, test_index) in train_test_index:\n",
    "    # Divide the data set into training data set and test data set\n",
    "    Train_Data = Data[train_index]\n",
    "    Train_Label = Label[train_index]\n",
    "    Test_Data = Data[test_index]\n",
    "    Test_Label = Label[test_index]\n",
    "\n",
    "    # Initialize the related result array\n",
    "    bgd_accuracy = []                           # One-time cross-validation accuracy of bgd algorithm\n",
    "    sgd_accuracy = []                           # One-time cross validation accuracy of sgd algorithm\n",
    "    mbgd_accuracy = []                          # One-time cross validation accuracy of mbgd algorithm\n",
    "    bgd_mse = []                                # One-time cross-validation MSE of bgd algorithm\n",
    "    sgd_mse = []                                # One-time cross-validation MSE of sgd algorithm\n",
    "    mbgd_mse = []                               # One-time cross-validation MSE of mbgd algorithm\n",
    "\n",
    "    # Initialize BPNN of various gradient algorithms\n",
    "    BGD = BPNN(input_n, hidden_n, output_n)\n",
    "    SGD = BPNN(input_n, hidden_n, output_n)\n",
    "    MBGD = BPNN(input_n, hidden_n, output_n)\n",
    "    BGD.Init(Train_Data,Train_Label)\n",
    "    SGD.Init(Train_Data, Train_Label)\n",
    "    MBGD.Init(Train_Data, Train_Label)\n",
    "\n",
    "    for iter in np.arange(Iteration):\n",
    "        BGD.train_dataset_BGD(learn_rate)\n",
    "        SGD.train_dataset_SGD(learn_rate)\n",
    "        MBGD.train_dataset_MBGD(learn_rate,mini_batch)\n",
    "        # BGD Algorithm result\n",
    "        predict_outputs = BGD.test(Test_Data)\n",
    "        bgd_accuracy.append(accuracy_score(Test_Label, predict_outputs))\n",
    "        bgd_mse.append(np.sum((Test_Label - predict_outputs) ** 2) / len(Test_Label))\n",
    "        if accuracy_score(Test_Label, predict_outputs)>best_accr:\n",
    "            best_accr=accuracy_score(Test_Label, predict_outputs)\n",
    "            best_model=BGD\n",
    "        # SGD Algorithm result\n",
    "        predict_outputs = SGD.test(Test_Data)\n",
    "        sgd_accuracy.append(accuracy_score(Test_Label, predict_outputs))\n",
    "        sgd_mse.append(np.sum((Test_Label - predict_outputs) ** 2) / len(Test_Label))\n",
    "        if accuracy_score(Test_Label, predict_outputs)>best_accr:\n",
    "            best_accr=accuracy_score(Test_Label, predict_outputs)\n",
    "            best_model=SGD\n",
    "        # MBGD Algorithm result\n",
    "        predict_outputs = BGD.test(Test_Data)\n",
    "        mbgd_accuracy.append(accuracy_score(Test_Label, predict_outputs))\n",
    "        mbgd_mse.append(np.sum((Test_Label - predict_outputs) ** 2) / len(Test_Label))\n",
    "        if accuracy_score(Test_Label, predict_outputs)>best_accr:\n",
    "            best_accr=accuracy_score(Test_Label, predict_outputs)\n",
    "            best_model=MBGD\n",
    "        if (iter+1) % 10 == 0:\n",
    "            print(\"%d,learn_rate=%f\" % (iter + 1,learn_rate))\n",
    "            print(\"BGD TA: %f,MSE：%f\" % (bgd_accuracy[iter], bgd_mse[iter]))\n",
    "            print(\"SGD TA: %f,MSE：%f\" % (sgd_accuracy[iter], sgd_mse[iter]))\n",
    "            print(\"MBGD TA：%f,MSE：%f\" % (mbgd_accuracy[iter], mbgd_mse[iter]))\n",
    "    BGD_Accuracy.append(list(bgd_accuracy))\n",
    "    SGD_Accuracy.append(list(sgd_accuracy))\n",
    "    MBGD_Accuracy.append(list(mbgd_accuracy))\n",
    "    BGD_MSE.append(list(bgd_mse))\n",
    "    SGD_MSE.append(list(sgd_mse))\n",
    "    MBGD_MSE.append(list(mbgd_mse))\n",
    "\n",
    "# Calculate and visualize the average test mean square error of the BP neural network model and the neural network model based on the intercellular environment\n",
    "col = [\"BGD\", \"SGD\",\"MBGD\"]\n",
    "BGD_MSE = np.average(np.array(BGD_MSE), 0)\n",
    "SGD_MSE = np.average(np.array(SGD_MSE), 0)\n",
    "MBGD_MSE = np.average(np.array(MBGD_MSE), 0)\n",
    "xticks = np.arange(1, len(BGD_MSE) + 1)\n",
    "error = [BGD_MSE,SGD_MSE,MBGD_MSE]\n",
    "error_result = Merge(error, col)\n",
    "error_result.to_excel(\"./MSE_iteration=%d_learn_rate=%f.xlsx\"%(Iteration,learn_rate))\n",
    "error_result.describe().to_excel(\"./_iteration=%d_learn_rate=%f.xlsx\"%(Iteration,learn_rate))\n",
    "plt.plot(xticks, BGD_MSE, 'c-')\n",
    "plt.plot(xticks, SGD_MSE, 'b-.')\n",
    "plt.plot(xticks, MBGD_MSE, 'r--')\n",
    "plt.legend(labels=col, loc=\"best\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Test MSE\")\n",
    "plt.xlim(1, len(BGD_MSE))\n",
    "plt.grid(True)\n",
    "plt.savefig(\"./MSE_iteration=%d_learn_rate=%f.jpg\"%(Iteration,learn_rate))\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Calculate and visualize the test accuracy of the BP neural network model and the neural network model based on the intercellular environment\n",
    "col = [\"BGD\", \"SGD\", \"MBGD\"]\n",
    "BGD_Accuracy = np.average(np.array(BGD_Accuracy), 0)\n",
    "SGD_Accuracy = np.average(np.array(SGD_Accuracy), 0)\n",
    "MBGD_Accuracy = np.average(np.array(MBGD_Accuracy), 0)\n",
    "acc = [BGD_Accuracy,SGD_Accuracy,MBGD_Accuracy]\n",
    "accuracy = Merge(acc, col)\n",
    "accuracy.to_excel(\"./_iteration=%d_learn_rate=%f.xlsx\"%(Iteration,learn_rate))\n",
    "accuracy.describe().to_excel(\"./_iteration=%d_learn_rate=%f.xlsx\"%(Iteration,learn_rate))\n",
    "xticks = np.arange(1, len(BGD_Accuracy) + 1)\n",
    "plt.plot(xticks, BGD_Accuracy, 'c-')\n",
    "plt.plot(xticks, SGD_Accuracy, 'b--')\n",
    "plt.plot(xticks, MBGD_Accuracy, 'r-.')\n",
    "plt.legend(labels=col, loc=\"best\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"./iteration=%d_learn_rate=%f.jpg\"%(Iteration,learn_rate))\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(best_accr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'test'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-d9867fada4e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"predict\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'test'"
     ]
    }
   ],
   "source": [
    "data[\"predict\"]=best_model.test(np.array(data[features]))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c5d84736ba45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-c2e3af77f5fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./result.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data.to_csv(\"./result.csv\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
